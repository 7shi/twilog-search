# data_csv.py

## なぜこの実装が存在するか

### データベース構築プロセスの簡略化
**Problem**: 現在のワークフローでは、CSVファイルからSQLiteデータベースを構築し、さらに複数の正規化段階を経る必要があった。22万件程度のデータでは、データベースの利点（永続化、複雑なクエリ）よりも構築コストの方が高く、シンプルな全件検索用途には過剰な仕組みだった。

**Solution**: CSVファイルを直接メモリに読み込むアプローチを採用。起動時の一括読み込みにより、データベース構築プロセスを完全に省略し、即座に検索可能な状態を実現した。22万件のデータはメモリ使用量的にも問題なく、高速アクセスが可能。

### 依存関係の削減とシンプル化
**Problem**: SQLiteを使用することで、データベースファイルの管理、インデックス作成、複数テーブルの結合など、検索機能には不要な複雑さが生じていた。また、conv.py、extract_users.py等の前処理スクリプトも必要で、セットアップが複雑だった。

**Solution**: CSVファイルのみを使用し、extract_users.pyのロジックを内包することで依存関係を最小限に抑制。Pythonの標準ライブラリ（csv、re）のみで実装し、外部データベースシステムと外部TSVファイルへの依存を完全に排除した。

### data_sqlite.pyとの完全な互換性確保
**Problem**: 既存のsearch.pyや他のコンポーネントがdata_sqlite.pyのインターフェースに依存しているため、新しいデータアクセス層でも同じインターフェースを提供する必要があった。

**Solution**: TwilogDataAccessクラス名と、load_user_data()、get_post_content()メソッドの戻り値形式を完全に一致させた。これにより、import文の変更のみで切り替え可能なドロップイン置換を実現した。

### メモリ効率的なデータ構造設計
**Problem**: 22万件のデータを効率的にメモリ上で管理し、高速な検索アクセスを提供する必要があった。単純な線形検索では性能が劣化する可能性があった。

**Solution**: 辞書ベースのpost_id→データマッピングを採用し、O(1)での高速アクセスを実現。CSVファイルから直接ユーザー情報を抽出することで、外部ファイル依存を排除し、効率的なデータ構造を設計した。

### extract_users.pyロジックの統合
**Problem**: ユーザー情報を取得するためにusers.tsvファイルが必要で、事前にextract_users.pyの実行が必要だった。これにより、セットアップが複雑になり、ファイル依存関係が増加していた。

**Solution**: extract_users.pyのURL解析ロジック（正規表現パターン、log_type優先度処理）をdata_csv.py内に統合。CSVファイルのみから直接ユーザー情報を抽出することで、外部ファイル依存を完全に排除し、真にシンプルな単一ファイルベースのデータアクセスを実現した。

### 重複データ処理の最適化
**Problem**: CSVデータの読み込み時と、ユーザー抽出時の両段階で同じpost_idの重複チェックが行われており、コードが冗長で処理効率が低下していた。

**Solution**: log_type優先度判定をCSV読み込み段階（`_load_csv_data`）に集約し、後段の`_extract_users_from_csv`では単純なユーザー抽出のみに特化させた。辞書の性質を活用してより早い段階で重複を解決することで、処理の効率化と コードの簡潔性を実現した。

### HTML文字実体参照のデコード処理
**Problem**: CSVファイルのcontentフィールドには`&lt;`、`&amp;`、`&gt;`等のHTML文字実体参照が含まれており、検索やベクトル化処理において期待通りの結果を得られない問題が発生していた。

**Solution**: CSVファイル読み込み時にPythonの標準ライブラリ`html.unescape()`を適用し、文字実体参照を適切な文字に変換。これにより、検索精度の向上とベクトル化処理の正確性を確保した。

### テキスト前処理機能の統合
**Problem**: ベクトル化処理で使用するテキスト前処理（URL・メンション除去）が別ファイル（preprocess_content.py）に分離されており、データアクセス層との連携が複雑だった。また、URL抽出機能が不足していた。

**Solution**: `strip_content`関数と`extract_urls`関数をdata_csv.pyに統合し、統一されたテキスト処理インターフェースを提供。同一の正規表現パターンを使用することで、処理の一貫性を保証し、ベクトル化処理との連携を簡素化した。

### ユーザーリスト保持機能の追加
**Problem**: ユーザー名の補完機能やレーベンシュタイン距離による類似ユーザー検索を実装するには、全ユーザー名のリストが必要だった。user_post_countsの辞書からkeysを取得する方法もあるが、頻繁なアクセスでは非効率で、他のコンポーネントでユーザーリストが必要になる度にkeysの変換処理が発生していた。

**Solution**: `_extract_users_from_csv`メソッドでuser_post_countsの構築と同時にuser_listを生成し、load_user_data()メソッドの戻り値に追加。これにより、ユーザーリストが事前に構築され、SearchEngineや補完機能で直接利用可能になった。メモリ使用量の増加は微小で、アクセス効率が大幅に向上。