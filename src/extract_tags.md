# タグ付けモジュール

## なぜこの実装が存在するか

### CSVベースアーキテクチャへの移行
**Problem**: 従来のSQLiteベース実装では、preprocess_contentテーブルを作成するために複雑なデータベース構築手順が必要であった。

**Solution**: data_csv.pyのstrip_content関数を利用して、CSVファイルから直接データを読み込み、リアルタイムで前処理を適用する構造に変更した。

### 意味的検索の強化が必要
**Problem**: ベクトル検索だけでは、検索クエリと直接的に類似した投稿しか発見できない。関連するトピックを持つが表現が異なる投稿を見逃し、検索の網羅性が不足する。

**Solution**: 投稿内容からタグを自動抽出し、タグ経由での関連投稿発見を可能にするハイブリッド検索システムの基盤を構築した。

### 手動タグ付けの現実的困難
**Problem**: 22万件の投稿に手動でタグを付与するのは現実的でない。また、一貫性のあるタグ体系を人間が維持するのは困難で、主観的なばらつきが生じる。

**Solution**: ローカルLLM（Ollama + Qwen3）による自動タグ付けシステムを実装。一貫した基準での大量データ処理を実現した。

### 構造化出力の安定性確保
**Problem**: LLMの出力は不安定で、期待した形式でないデータが返される可能性がある。また、モデルによって指示への準拠度が異なり、信頼性の低い結果となる。

**Solution**: 20250704-structured-output.mdの実験結果に基づき、プロンプトでキーと指示を明確に対応付ける方式を採用。Pydanticスキーマによる型安全性も確保した。

### 大量データの安全な処理
**Problem**: 22万件の連続処理では、ネットワーク障害やシステムクラッシュで数時間の処理が無駄になるリスクがある。また、処理途中での進捗確認も困難。

**Solution**: JSONL形式での1件ごと追記保存とチェックポイント機能を実装。処理済みpost_idを起動時に読み込み、未処理データのみを効率的に処理する設計とした。

### ファイルサイズとメモリ効率の最適化
**Problem**: 処理済みテキストを含めると22万件でファイルサイズが巨大化し、後続処理でのメモリ使用量も増大する。また、タグ検索では元テキストは不要。

**Solution**: フラットな{post_id, reasoning, summary, tags}構造を採用。必要最小限の情報のみを保存し、ファイルサイズとメモリ効率を最適化した。

### 分割ファイル格納によるGit管理効率化
**Problem**: 単一のJSONLファイルでは22万件処理完了時に巨大ファイルとなり、Git管理が困難。差分管理も非効率で、部分的な読み込みもできない。

**Solution**: embeddings/と同様に1000件ごとの分割格納を採用。`tags/0000.jsonl`、`tags/0001.jsonl`等の構造により、各ファイルを約500KB以内に抑制。Git LFS不要でありながら効率的な管理と部分読み込みを実現した。

### ローカル実行環境での処理効率
**Problem**: クラウドAPIでは22万件の処理コストが高額になり、また外部依存による処理速度の不安定性がある。

**Solution**: Ollamaでのローカル実行を前提とした設計。GPU環境での効率的なシーケンシャル処理により、コストを抑えつつ安定した処理速度を実現した。

### スキップ処理での正確な件数制御
**Problem**: 中断・再開機能でスキップされた件数がlimit指定に影響し、意図した件数分の処理が行われない。例えば`--limit 2`で既存1件をスキップすると、実際には1件しか処理されない問題が発生する。

**Solution**: 起動時に既存データ件数を取得し、CSV読み込みのlimitに加算する方式を採用。`adjusted_limit = limit + existing_count`により、スキップ件数に関係なく指定した件数分の新規処理を保証した。

### 処理時間の実測による運用計画
**Problem**: 22万件規模の処理時間が予測できないと、運用スケジュールや処理戦略の計画が困難。また、リソース使用量の見積もりも不正確になる。

**Solution**: 段階的な実測による処理性能の確認。GPU環境（Ollama + Qwen3:4b）での実測結果に基づく運用計画を策定。

## 処理時間実測記録（SQLiteベース時代）
- 100件: 3分45秒 (2.25秒/件)
- 800件: 33分45秒 (2.531秒/件)
- 900件: 37分49.850秒 (2.522秒/件)
- 2000件: 83分59.206秒 (2.520秒/件)

平均処理速度: 2.521秒/件で安定。全225,827件 × 2.521秒/件 = 569,310秒 (158.1時間, 6.6日間)。

## CSVベース移行の影響
- **データ読み込み**: processed_contentテーブル → CSVファイル + strip_content処理
- **処理速度**: 前処理のオーバーヘッドが追加されるが、LLM処理がボトルネックのため全体への影響は軽微
- **セットアップ**: SQLiteデータベース構築不要で即座実行可能