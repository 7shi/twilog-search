# batch_merge.py

## なぜこの実装が存在するか

### バッチ処理結果の統合とデータ抽出の必要性
**Problem**: Gemini APIのバッチ処理結果が複数のJSONLファイルに分散しており、各ファイルの`candidates[0].content.parts[0].text`フィールドに格納されたJSON形式のデータを効率的に抽出し、後続処理で使いやすい形式に統合する必要があった。

**Solution**: 複数のJSONLファイルを一括処理し、テキストフィールドをJSONとしてパースして、キー（post_id）とともに統合したJSONLファイルを生成する専用スクリプトを作成した。

### 大量データの効率的な処理とメモリ管理
**Problem**: 数万件のデータを処理する際、進捗が見えずユーザーエクスペリエンスが悪く、また処理中のメモリ使用量やエラーの発生状況が把握できなかった。

**Solution**: tqdmライブラリを使用した進捗表示機能と、全データをメモリに読み込んでから一括処理する方式を採用。エラーが発生した行は無視してスキップし、処理を継続する堅牢な実装とした。

### データの整合性とソート要件
**Problem**: 複数ファイルから抽出されたデータが時系列やキーの順序に依存する後続処理で使用されるため、データの並び順が重要だった。また、キーの型が文字列として扱われると数値ソートが正しく機能しなかった。

**Solution**: キーを整数型に変換し、全データ読み込み後にキーの昇順でソートしてから出力する仕組みを実装。これにより、後続処理でデータの順序性を保証できるようにした。

### 柔軟な出力形式と統合データ構造
**Problem**: 抽出したJSONデータとキー情報を別々に管理すると、後続処理でデータの対応関係を維持するのが困難で、処理ロジックが複雑になる問題があった。

**Solution**: `{"key": post_id, **json_data}`形式でキーとデータを統合し、JSONLファイルとして出力する方式を採用。これにより、単一のファイルで完結したデータ構造を実現し、後続処理を簡素化した。

### 堅牢なエラーハンドリングとファイル処理
**Problem**: 大量のJSONLファイルを処理する際、一部のファイルが破損していたり、JSON構造が期待と異なる場合に処理が停止してしまい、有効なデータまで失われる可能性があった。

**Solution**: 各処理段階でtry-catch文を使用した多層的なエラーハンドリングを実装。ファイル読み込みエラー、JSON解析エラー、データ構造エラーを個別に処理し、エラーが発生した項目のみをスキップして処理を継続する仕組みを構築した。

### 必須出力オプションによる意図的なファイル管理
**Problem**: 出力ファイルを指定しないと標準出力に大量のデータが出力され、ターミナルが使用不能になったり、意図しない場所にデータが流れる問題があった。

**Solution**: `-o/--output`オプションを必須として設定し、出力先を明示的に指定することを強制。これにより、意図しないデータ出力を防ぎ、ファイル管理を適切に行えるようにした。

### LLM暴走検出とデータ品質管理
**Problem**: LLMが無限ループや文字繰り返しで異常に長いレスポンスを生成し、JSON解析エラーやメモリ使用量増大の原因となっていた。これらの暴走ケースを適切に分類できずに有効データとして扱ってしまう問題があった。

**Solution**: テキスト長による暴走判定（10000文字以上）を実装し、明らかに異常なレスポンスを事前に除外する仕組みを構築。暴走ケースは別カテゴリとして分類し、データ品質の向上を実現した。

### JSON補正機能による有効データの最大活用
**Problem**: LLMが正常なJSONデータを````json ... ````マークダウン形式で出力するケースが頻発し、これらの有効なデータがJSON解析エラーとして失われていた。

**Solution**: マークダウン記号を除去してJSON解析を再試行する補正機能を実装。補正成功件数は別途集計し、エラーとは区別して管理することで、データ活用率を向上させた。

### クラス設計による処理の構造化と拡張性
**Problem**: 巨大な関数による処理が保守性を悪化させ、パラメーターの調整や新機能の追加が困難になっていた。また、統計データの管理が複雑で、エラー分類の追加や変更が困難だった。

**Solution**: `JsonlProcessor`クラスを設計し、処理ロジックをメソッドに分離。LLM暴走判定閾値などのパラメーターをフィールド化し、統計管理を明確に分離した。これにより、コードの再利用性と拡張性を大幅に向上させた。

### 詳細な統計情報と診断機能
**Problem**: 処理結果の詳細が不明で、エラーの原因特定や処理品質の評価が困難だった。また、開発時とプロダクション利用時で必要な情報レベルが異なっていた。

**Solution**: 文字数分布ヒストグラムとエラー詳細表示機能を実装し、`--verbose`オプションで出力レベルを制御可能にした。これにより、通常利用時は簡潔な出力、問題調査時は詳細な診断情報を提供できるようにした。

### 構造化出力のスキーマ検証と品質保証
**Problem**: Gemini APIの構造化出力が期待される`reasoning`、`summary`、`tags`の3項目を正確に含んでいるかの検証が不十分で、フィールドの過不足があった場合に後続処理でエラーが発生する可能性があった。また、構造化出力の品質を定量的に評価する仕組みが不足していた。

**Solution**: JSONデータが正確に3項目のみを含むかを検証する`_validate_fields`メソッドを実装し、フィールド過不足を独立したエラーカテゴリとして分類。これにより、構造化出力の品質を監視し、スキーマに準拠しないデータを確実に除外する仕組みを構築した。実際の運用では225,674件すべてがスキーマに準拠していることが確認され、Gemini APIの構造化出力の高い信頼性が実証された。