# タグ付けモジュール

## なぜこの実装が存在するか

### LLMコストを考慮した1件ずつ保存アーキテクチャ
**Problem**: vectorize.pyのチャンク処理をそのまま採用すると、LLM処理失敗時に1チャンク分の処理が無駄になり、コストが高額なLLM処理には適さない。

**Solution**: vectorize.pyの設計思想（重複チェック、進捗表示、ファイル管理）を維持しつつ、LLM処理は1件ずつ実行し即座に保存する方式を採用。処理コストを最小限に抑えながら安全性を確保した。

### CSVベースアーキテクチャへの移行
**Problem**: 従来のSQLiteベース実装では、preprocess_contentテーブルを作成するために複雑なデータベース構築手順が必要であった。

**Solution**: data_csv.pyのstrip_content関数を利用して、CSVファイルから直接データを読み込み、リアルタイムで前処理を適用する構造に変更した。

### 意味的検索の強化が必要
**Problem**: ベクトル検索だけでは、検索クエリと直接的に類似した投稿しか発見できない。関連するトピックを持つが表現が異なる投稿を見逃し、検索の網羅性が不足する。

**Solution**: 投稿内容からタグを自動抽出し、タグ経由での関連投稿発見を可能にするハイブリッド検索システムの基盤を構築した。

### 手動タグ付けの現実的困難
**Problem**: 22万件の投稿に手動でタグを付与するのは現実的でない。また、一貫性のあるタグ体系を人間が維持するのは困難で、主観的なばらつきが生じる。

**Solution**: ローカルLLM（Ollama + Qwen3）による自動タグ付けシステムを実装。一貫した基準での大量データ処理を実現した。

### 構造化出力の安定性確保
**Problem**: LLMの出力は不安定で、期待した形式でないデータが返される可能性がある。また、モデルによって指示への準拠度が異なり、信頼性の低い結果となる。

**Solution**: 20250704-structured-output.mdの実験結果に基づき、プロンプトでキーと指示を明確に対応付ける方式を採用。Pydanticスキーマによる型安全性も確保した。

### インテリジェントなファイル管理による効率的な分割保存
**Problem**: 単純な1件ずつ保存では適切なファイル分割ができず、また中間ファイルが削除された場合に番号の隙間が生じて管理が複雑になる。

**Solution**: 既存ファイルの行数を事前に収集し、chunk_size未満のファイルを優先的に利用する`get_target_file_index`関数を実装。中間が抜けている場合も適切に番号を再利用し、効率的なファイル管理を実現した。

### ファイルサイズとメモリ効率の最適化
**Problem**: 処理済みテキストを含めると22万件でファイルサイズが巨大化し、後続処理でのメモリ使用量も増大する。また、タグ検索では元テキストは不要。

**Solution**: フラットな{post_id, reasoning, summary, tags}構造を採用。必要最小限の情報のみを保存し、ファイルサイズとメモリ効率を最適化した。

### 分割ファイル格納によるGit管理効率化
**Problem**: 単一のJSONLファイルでは22万件処理完了時に巨大ファイルとなり、Git管理が困難。差分管理も非効率で、部分的な読み込みもできない。

**Solution**: embeddings/と同様に1000件ごとの分割格納を採用。`tags/0000.jsonl`、`tags/0001.jsonl`等の構造により、各ファイルを約500KB以内に抑制。Git LFS不要でありながら効率的な管理と部分読み込みを実現した。

### 重複データの確実な検出
**Problem**: 処理済みpost_idの重複が発生すると、データの整合性が損なわれ、後続の検索処理で予期しない結果を生む。

**Solution**: vectorize.pyと同様の厳密な重複チェック機能を実装。既存JSONLファイル読み込み時に重複post_idを検出し、発見時は処理を停止してエラー報告する安全機構を追加した。


### テスト実行を支援するlimit機能
**Problem**: 22万件の全データ処理は時間がかかるため、アルゴリズム検証やパラメータ調整時に小規模なテストができず、開発効率が低下する。

**Solution**: `--limit`オプションを追加し、処理件数を制限できる機能を実装。既存データをスキップした後の残りデータに対してのみlimit制限を適用し、段階的な処理確認を可能にした。

### 処理時間の実測による運用計画
**Problem**: 22万件規模の処理時間が予測できないと、運用スケジュールや処理戦略の計画が困難。また、リソース使用量の見積もりも不正確になる。

**Solution**: datetimeモジュールによる実行時間計測機能を追加。処理開始・終了時刻と所要時間を秒単位で記録し、vectorize.pyと同様の運用データ蓄積を可能にした。

### 必須引数による安全な実行環境
**Problem**: デフォルト値を持つCSVファイル引数では、意図しないファイルが処理される危険性があり、また利用可能なオプションが不明瞭になる。

**Solution**: `csv_file`をデフォルト値なしの必須引数とし、引数なし実行時にヘルプ表示を強制。ユーザーが適切な使用方法を確認してから実行する安全な設計とした。

## 処理時間実測記録（SQLiteベース時代）
- 100件: 3分45秒 (2.25秒/件)
- 800件: 33分45秒 (2.531秒/件)
- 900件: 37分49.850秒 (2.522秒/件)
- 2000件: 83分59.206秒 (2.520秒/件)

平均処理速度: 2.521秒/件で安定。全225,827件 × 2.521秒/件 = 569,310秒 (158.1時間, 6.6日間)。

## CSVベース移行の影響
- **データ読み込み**: processed_contentテーブル → CSVファイル + strip_content処理
- **処理速度**: 前処理のオーバーヘッドが追加されるが、LLM処理がボトルネックのため全体への影響は軽微
- **セットアップ**: SQLiteデータベース構築不要で即座実行可能