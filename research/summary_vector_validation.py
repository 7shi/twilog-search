#!/usr/bin/env python3
"""
è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

LLMã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ãŒåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¦‹ã¦åŸæ–‡ã‚’é©åˆ‡ã«åæ˜ ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
è¦ç´„ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã—ã¦ã€100ä½ä»¥å†…ã«å…ƒã®æŠ•ç¨¿ãŒç¾ã‚Œã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹ã€‚
"""
import asyncio
import sys
import random
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from batch_reader import BatchReader
from twilog_client import TwilogClient
from settings import SearchSettings


class SummaryVectorValidator:
    """è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, batch_file: Path, websocket_url: str = "ws://localhost:8765"):
        """
        åˆæœŸåŒ–
        
        Args:
            batch_file: batch/results.jsonlãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            websocket_url: WebSocketã‚µãƒ¼ãƒãƒ¼ã®URL
        """
        self.batch_reader = BatchReader(batch_file)
        self.client = TwilogClient(websocket_url)
        
    def extract_fixed_samples(self, seed: int, sample_size: int) -> List[int]:
        """å›ºå®šã‚·ãƒ¼ãƒ‰ã§å†ç¾å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºï¼ˆè¦ç´„ã‚ã‚Šã®ã¿ï¼‰"""
        random.seed(seed)
        
        # è¦ç´„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æŠ•ç¨¿ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        summaries_with_data = []
        for post_id, data in self.batch_reader.summaries_data.items():
            if data.get("summary", "").strip():  # è¦ç´„ãŒç©ºã§ãªã„å ´åˆã®ã¿
                summaries_with_data.append(post_id)
        
        print(f"è¦ç´„ã‚ã‚ŠæŠ•ç¨¿: {len(summaries_with_data):,}ä»¶ / å…¨æŠ•ç¨¿: {len(self.batch_reader.summaries_data):,}ä»¶")
        
        # è¦ç´„ã‚ã‚ŠæŠ•ç¨¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        actual_sample_size = min(sample_size, len(summaries_with_data))
        samples = random.sample(summaries_with_data, actual_sample_size)
        
        print(f"å›ºå®šã‚·ãƒ¼ãƒ‰({seed})ã§{actual_sample_size}ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºå®Œäº†ï¼ˆè¦ç´„ã‚ã‚Šã®ã¿ï¼‰")
        return samples

    async def validate_summary_vector_search(self, sample_size: int, seed: int, mode: str = "content") -> Dict:
        """
        è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§ã‚’æ¤œè¨¼
        
        Args:
            sample_size: æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
            mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆcontent, reasoning, summaryï¼‰
            
        Returns:
            æ¤œè¨¼çµæœã®çµ±è¨ˆæƒ…å ±
        """
        print(f"ğŸ” è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§æ¤œè¨¼ã‚’é–‹å§‹")
        print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size}, ã‚·ãƒ¼ãƒ‰: {seed}, ãƒ¢ãƒ¼ãƒ‰: {mode}")
        
        # BatchReaderã‚’åˆæœŸåŒ–
        self.batch_reader.initialize()
        summaries_data = self.batch_reader.summaries_data
        
        if not summaries_data:
            raise ValueError("ãƒãƒƒãƒå‡¦ç†çµæœãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
        
        print(f"ç·è¦ç´„ãƒ‡ãƒ¼ã‚¿æ•°: {len(summaries_data):,}ä»¶")
        
        # å›ºå®šã‚·ãƒ¼ãƒ‰ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆtag_reverse_search_2.pyå‚ç…§ï¼‰
        sampled_post_ids = self.extract_fixed_samples(seed, sample_size)
        actual_sample_size = len(sampled_post_ids)
        
        # æ¤œè¨¼å®Ÿè¡Œ
        results = []
        hit_counts = {"top_10": 0, "top_20": 0, "top_50": 0, "top_100": 0}
        ranks = []
        
        for i, post_id in enumerate(sampled_post_ids, 1):
            summary = summaries_data[post_id]["summary"]
            print(f"\n[{i}/{actual_sample_size}] æŠ•ç¨¿ID: {post_id}")
            print(f"è¦ç´„: {summary[:100]}...")
            
            try:
                # è¦ç´„ã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆæŒ‡å®šãƒ¢ãƒ¼ãƒ‰, top_k=100ï¼‰
                search_result = await self.client.search_similar(
                    query_text=summary,
                    search_settings=SearchSettings(initial_top_k=100),
                    mode=mode
                )
                
                # å…ƒæŠ•ç¨¿ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ç¢ºèª
                rank = self._find_post_rank(search_result, post_id)
                
                if rank is not None:
                    ranks.append(rank)
                    print(f"âœ… å…ƒæŠ•ç¨¿ãŒ {rank}ä½ ã§ç™ºè¦‹")
                    
                    # ãƒ’ãƒƒãƒˆç‡ã‚«ã‚¦ãƒ³ãƒˆ
                    if rank <= 10:
                        hit_counts["top_10"] += 1
                    if rank <= 20:
                        hit_counts["top_20"] += 1
                    if rank <= 50:
                        hit_counts["top_50"] += 1
                    if rank <= 100:
                        hit_counts["top_100"] += 1
                else:
                    print("âŒ å…ƒæŠ•ç¨¿ãŒ100ä½ä»¥å†…ã«è¦‹ã¤ã‹ã‚‰ãš")
                
                results.append({
                    "post_id": post_id,
                    "summary": summary,
                    "rank": rank,
                    "found": rank is not None
                })
                
            except Exception as e:
                print(f"âš ï¸  æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                results.append({
                    "post_id": post_id,
                    "summary": summary,
                    "rank": None,
                    "found": False,
                    "error": str(e)
                })
        
        # çµ±è¨ˆè¨ˆç®—
        stats = self._calculate_statistics(results, hit_counts, actual_sample_size)
        
        # çµæœå‡ºåŠ›ï¼ˆã‚µãƒ³ãƒ—ãƒ«è©³ç´°ã‚‚è¡¨ç¤ºï¼‰
        self._print_sample_details(sampled_post_ids, results, mode)
        self._print_results(stats)
        
        return {
            "statistics": stats,
            "detailed_results": results,
            "sample_size": actual_sample_size,
            "seed": seed
        }
    
    def _find_post_rank(self, search_results: List[Dict], target_post_id: int) -> Optional[int]:
        """
        æ¤œç´¢çµæœã‹ã‚‰æŒ‡å®šæŠ•ç¨¿ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—
        
        Args:
            search_results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
            target_post_id: å¯¾è±¡æŠ•ç¨¿ID
            
        Returns:
            ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆ1-basedï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        for rank, result in enumerate(search_results, 1):
            # æ§˜ã€…ãªå¯èƒ½æ€§ã®ã‚ã‚‹ã‚­ãƒ¼æ§‹é€ ã«å¯¾å¿œ
            post_id = None
            if "post" in result and "post_id" in result["post"]:
                post_id = result["post"]["post_id"]
            elif "post" in result and "id" in result["post"]:
                post_id = result["post"]["id"]
            elif "id" in result:
                post_id = result["id"]
            elif "post_id" in result:
                post_id = result["post_id"]
            
            if post_id == target_post_id:
                return rank
        return None
    
    def _calculate_statistics(self, results: List[Dict], hit_counts: Dict[str, int], sample_size: int) -> Dict:
        """
        æ¤œè¨¼çµæœã®çµ±è¨ˆã‚’è¨ˆç®—
        
        Args:
            results: è©³ç´°çµæœã®ãƒªã‚¹ãƒˆ
            hit_counts: ãƒ’ãƒƒãƒˆæ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ
            sample_size: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
            
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        # ç™ºè¦‹ã•ã‚ŒãŸæŠ•ç¨¿ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ã¿æŠ½å‡º
        found_ranks = [r["rank"] for r in results if r["found"]]
        
        stats = {
            "total_samples": sample_size,
            "found_count": len(found_ranks),
            "not_found_count": sample_size - len(found_ranks),
            "found_rate": len(found_ranks) / sample_size * 100,
            "hit_rates": {
                "top_10": hit_counts["top_10"] / sample_size * 100,
                "top_20": hit_counts["top_20"] / sample_size * 100,
                "top_50": hit_counts["top_50"] / sample_size * 100,
                "top_100": hit_counts["top_100"] / sample_size * 100,
            }
        }
        
        if found_ranks:
            stats["rank_statistics"] = {
                "mean": np.mean(found_ranks),
                "median": np.median(found_ranks),
                "min": min(found_ranks),
                "max": max(found_ranks),
                "best_rank_count": sum(1 for r in found_ranks if r == 1)
            }
        
        return stats
    
    def _print_sample_details(self, sampled_post_ids: List[int], results: List[Dict], mode: str) -> None:
        """
        ã‚µãƒ³ãƒ—ãƒ«è©³ç´°ã‚’è¡¨ç¤ºï¼ˆtag_reverse_search_2.pyå½¢å¼ï¼‰
        
        Args:
            sampled_post_ids: ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿IDã®ãƒªã‚¹ãƒˆ
            results: æ¤œè¨¼çµæœã®ãƒªã‚¹ãƒˆ
        """
        print("\n" + "="*60)
        print(f"ğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«è©³ç´° ({len(sampled_post_ids)}ä»¶ã®å›ºå®šã‚·ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«)")
        print("="*60)
        
        print(f"\nã‚µãƒ³ãƒ—ãƒ«åˆ¥é †ä½ï¼ˆè¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã®å…ƒæŠ•ç¨¿ãƒ©ãƒ³ã‚­ãƒ³ã‚°, mode={mode}):")
        print(f"{'ID':<20s} {'é †ä½':<8s} {'ç™ºè¦‹çŠ¶æ³':<10s}")
        print("-" * 40)
        
        for i, (post_id, result) in enumerate(zip(sampled_post_ids, results)):
            rank_str = f"{result['rank']}ä½" if result['rank'] is not None else "åœå¤–"
            found_str = "âœ…ç™ºè¦‹" if result['found'] else "âŒåœå¤–"
            print(f"{str(post_id):<20s} {rank_str:<8s} {found_str:<10s}")

    def _print_results(self, stats: Dict) -> None:
        """
        æ¤œè¨¼çµæœã‚’å‡ºåŠ›
        
        Args:
            stats: çµ±è¨ˆæƒ…å ±
        """
        print("\n" + "="*60)
        print("ğŸ“Š è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§æ¤œè¨¼çµæœ")
        print("="*60)
        
        print(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['total_samples']:,}ä»¶")
        print(f"ç™ºè¦‹ä»¶æ•°: {stats['found_count']:,}ä»¶")
        print(f"æœªç™ºè¦‹ä»¶æ•°: {stats['not_found_count']:,}ä»¶")
        print(f"ç™ºè¦‹ç‡: {stats['found_rate']:.1f}%")
        
        print("\nğŸ¯ ãƒ’ãƒƒãƒˆç‡:")
        for key, rate in stats["hit_rates"].items():
            rank_range = key.replace("top_", "")
            print(f"  {rank_range}ä½ä»¥å†…: {rate:.1f}%")
        
        if "rank_statistics" in stats:
            rank_stats = stats["rank_statistics"]
            print("\nğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµ±è¨ˆ (ç™ºè¦‹ã•ã‚ŒãŸæŠ•ç¨¿ã®ã¿):")
            print(f"  å¹³å‡é †ä½: {rank_stats['mean']:.1f}ä½")
            print(f"  ä¸­å¤®å€¤é †ä½: {rank_stats['median']:.1f}ä½")
            print(f"  æœ€é«˜é †ä½: {rank_stats['min']}ä½")
            print(f"  æœ€ä½é †ä½: {rank_stats['max']}ä½")
            print(f"  1ä½ç²å¾—æ•°: {rank_stats['best_rank_count']}ä»¶")
        
        print("\nğŸ’¡ è¦ç´„å“è³ªè©•ä¾¡:")
        found_rate = stats['found_rate']
        top_10_rate = stats["hit_rates"]["top_10"]
        
        if found_rate >= 95:
            quality = "å„ªç§€"
            icon = "ğŸŸ¢"
        elif found_rate >= 85:
            quality = "è‰¯å¥½"
            icon = "ğŸŸ¡"
        elif found_rate >= 70:
            quality = "æ™®é€š"
            icon = "ğŸŸ "
        else:
            quality = "è¦æ”¹å–„"
            icon = "ğŸ”´"
        
        print(f"{icon} è¦ç´„å“è³ª: {quality}")
        print(f"  - 100ä½ä»¥å†…ç™ºè¦‹ç‡: {found_rate:.1f}%")
        print(f"  - 10ä½ä»¥å†…é«˜ç²¾åº¦ç‡: {top_10_rate:.1f}%")


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹åŸæ–‡åæ˜ æ€§æ¤œè¨¼")
    parser.add_argument("-f", "--batch-file", type=Path, 
                       default=Path("batch/results.jsonl"),
                       help="ãƒãƒƒãƒå‡¦ç†çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("-n", "--sample-size", type=int, default=50,
                       help="æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50)")
    parser.add_argument("-s", "--seed", type=int, default=42,
                       help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42)")
    parser.add_argument("-u", "--url", default="ws://localhost:8765",
                       help="WebSocketã‚µãƒ¼ãƒãƒ¼URL (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ws://localhost:8765)")
    parser.add_argument("-m", "--mode", default="content",
                       choices=["content", "reasoning", "summary", "average", "maximum", "minimum"],
                       help="æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: content)")
    
    args = parser.parse_args()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not args.batch_file.exists():
        print(f"âŒ ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.batch_file}")
        return 1
    
    try:
        validator = SummaryVectorValidator(args.batch_file, args.url)
        await validator.validate_summary_vector_search(args.sample_size, args.seed, args.mode)
        return 0
    except Exception as e:
        print(f"âŒ æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)