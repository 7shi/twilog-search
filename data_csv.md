# data_csv.py

## なぜこの実装が存在するか

### データベース構築プロセスの簡略化
**Problem**: 現在のワークフローでは、CSVファイルからSQLiteデータベースを構築し、さらに複数の正規化段階を経る必要があった。22万件程度のデータでは、データベースの利点（永続化、複雑なクエリ）よりも構築コストの方が高く、シンプルな全件検索用途には過剰な仕組みだった。

**Solution**: CSVファイルを直接メモリに読み込むアプローチを採用。起動時の一括読み込みにより、データベース構築プロセスを完全に省略し、即座に検索可能な状態を実現した。22万件のデータはメモリ使用量的にも問題なく、高速アクセスが可能。

### 依存関係の削減とシンプル化
**Problem**: SQLiteを使用することで、データベースファイルの管理、インデックス作成、複数テーブルの結合など、検索機能には不要な複雑さが生じていた。また、conv.py、extract_users.py等の前処理スクリプトも必要で、セットアップが複雑だった。

**Solution**: CSVファイルのみを使用し、extract_users.pyのロジックを内包することで依存関係を最小限に抑制。Pythonの標準ライブラリ（csv、re）のみで実装し、外部データベースシステムと外部TSVファイルへの依存を完全に排除した。

### data_sqlite.pyとの完全な互換性確保
**Problem**: 既存のsearch.pyや他のコンポーネントがdata_sqlite.pyのインターフェースに依存しているため、新しいデータアクセス層でも同じインターフェースを提供する必要があった。

**Solution**: TwilogDataAccessクラス名と、load_user_data()、get_post_content()メソッドの戻り値形式を完全に一致させた。これにより、import文の変更のみで切り替え可能なドロップイン置換を実現した。

### メモリ効率的なデータ構造設計
**Problem**: 22万件のデータを効率的にメモリ上で管理し、高速な検索アクセスを提供する必要があった。単純な線形検索では性能が劣化する可能性があった。

**Solution**: 辞書ベースのpost_id→データマッピングを採用し、O(1)での高速アクセスを実現。CSVファイルから直接ユーザー情報を抽出することで、外部ファイル依存を排除し、効率的なデータ構造を設計した。

### extract_users.pyロジックの統合
**Problem**: ユーザー情報を取得するためにusers.tsvファイルが必要で、事前にextract_users.pyの実行が必要だった。これにより、セットアップが複雑になり、ファイル依存関係が増加していた。

**Solution**: extract_users.pyのURL解析ロジック（正規表現パターン、log_type優先度処理）をdata_csv.py内に統合。CSVファイルのみから直接ユーザー情報を抽出することで、外部ファイル依存を完全に排除し、真にシンプルな単一ファイルベースのデータアクセスを実現した。

### HTML文字実体参照のデコード処理
**Problem**: CSVファイルのcontentフィールドには`&lt;`、`&amp;`、`&gt;`等のHTML文字実体参照が含まれており、検索やベクトル化処理において期待通りの結果を得られない問題が発生していた。

**Solution**: CSVファイル読み込み時にPythonの標準ライブラリ`html.unescape()`を適用し、文字実体参照を適切な文字に変換。これにより、検索精度の向上とベクトル化処理の正確性を確保した。

### テキスト前処理機能の統合
**Problem**: ベクトル化処理で使用するテキスト前処理（URL・メンション除去）が別ファイル（preprocess_content.py）に分離されており、データアクセス層との連携が複雑だった。また、URL抽出機能が不足していた。

**Solution**: `strip_content`関数と`extract_urls`関数をdata_csv.pyに統合し、統一されたテキスト処理インターフェースを提供。同一の正規表現パターンを使用することで、処理の一貫性を保証し、ベクトル化処理との連携を簡素化した。